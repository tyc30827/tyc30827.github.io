{
  "personalInfo": {
    "name": "Willie Huang",
    "role": "Data / MLOps Engineer",
    "profilePicture": "/images/willie.jpg",
    "email": "tyc30827@gmail.com",
    "socials": {
      "linkedin": "https://www.linkedin.com/in/willie-huang-ba79a9199/",
      "github": "https://github.com/tyc30827"
    }
  },
  "about": {
    "description": "A highly passionate Data Engineer with 5.5 years of hands-on experience in building robust, scalable data infrastructures. I excel at optimizing data pipelines and transforming complex, raw data into actionable insights. Driven by curiosity about data science, I continuously embrace new technologies to solve real-world data challenges and deliver high-impact business solutions."
  },
  "experience": [
    {
      "id": 1,
      "role": "Senior Data Engineer / MLOps Engineer",
      "company": "XRSpace Co., Ltd.",
      "duration": "2024/8 - Present",
      "description": [
          "Host all of the development about the AI-labeling system for a social media platform, integrating LLM (AWS Bedrock, PydanticAI), Flask, Kafka, and Dagster to establish a complete data processing pipeline.",
          "Engineered an end-to-end Avatar parts data pipeline supporting the critical Japan app launch in 2 sprints. This involved coordinating 7 cross-functional teams and tackling the complex challenge of parsing XRS binary files from S3/MongoDB into structured data. All BigQuery transformation logic was governed by dbt for testing and reliability, culminating in a Looker Studio dashboard for PM and Marketing teams.",
          "Drive data platform architecture optimization, system design, and operation of core infrastructure (Airbyte managed via Terraform, dbt, Dagster, BigQuery), achieving full automation and standardization of data processing flows.",
          "Optimize complex SQL queries and BigQuery data structure (clustering/partitioning), successfully reducing BigQuery query costs and computational usage by 90%.",
          "Led and mentored 3 junior engineers on fundamental data engineering concepts, system design, and technical culture, significantly enhancing the team's overall technical proficiency and delivery capabilities.",
          "Designed and implemented a Proof-of-Concept (PoC) for a Multi-modal RAG system via LlamaIndex.",
          "Successfully integrated a Structured Data RAG system into the main product, improving csv data retrieval accuracy for core features."
      ]
    },
    {
      "id": 2,
      "role": "Senior Data Engineer",
      "company": "CyberLink Corp.",
      "duration": "2023/10 - 2024/8",
      "description": [
        "Text mining for customer feedback data",
        "Data pipeline used for collecting total mobile install data from various datasource",
        "Data migration of existed projects handled by our data team"
      ]
    },
    {
      "id": 3,
      "role": "Data Scientist and Engineer",
      "company": "CyberLink Corp.",
      "duration": "2020/6 - 2023/10",
      "description": [
        "Data pipeline, processing hundreds of millions of records/GB-level data through Airflow for ETL/ELT.",
        "Data warehouse, performing near real-time analysis on big data.",
        "Data visualization using tools like Superset and Tableau.",
        "Information retrieval and content labeling from templates.",
        "End-to-end Personalized recommendation system in our main product: Promeo.",
        "GNN-based user preference system PoC."
      ]
    }
  ],
  "education": [
    {
      "id": 1,
      "degree": "Master of Science",
      "school": "National Chiao Tung University, Institute of Information Management",
      "year": "2017/9 - 2019/8"
    },
    {
      "id": 2,
      "degree": "Bachelor of Science",
      "school": "National Cheng Kung University, Department of Industrial and Information Management",
      "year": "2013/9 - 2017/6"
    }
  ],
  "certifications": [
    {
      "id": 1,
      "title": "TOEIC",
      "issuer": "ETS",
      "date": "",
      "score": "845"
    }
  ],
  "publications": [
    {
      "id": 1,
      "title": "Attentive gated graph sequence neural network-based time-series information fusion for financial trading",
      "conference": "Information Fusion",
      "date": "2022",
      "link": "https://www.sciencedirect.com/science/article/abs/pii/S1566253522001750",
      "description": "The first author of a paper in top-tier computer science journal, which has been cited for > 50 times."
    }
  ],
  "techStack": [
    {
        "category": "Programming Languages",
        "skills": ["Python", "SQL", "Java", "C++/C"]
    },
    {
        "category": "Databases / Data Warehouse",
        "skills": ["PostgreSQL", "MongoDB", "BigQuery", "AWS Athena", "AWS S3/MinIO", "DuckDB", "MySQL", "SQL Server"]
    },
    {
        "category": "Data Engineering",
        "skills": ["Airflow", "Dagster", "Airbyte", "dbt (Data Build Tool)", "AWS Athena", "Apache Kafka"]
    },
    {
        "category": "Data Visualization & BI",
        "skills": ["GA4", "Looker Studio", "Superset", "Tableau"]
    },
    {
        "category": "Backend",
        "skills": ["Flask", "Pydantic", "FastAPI"]
    },
    {
        "category": "Infrastructure",
        "skills": ["Terraform", "Docker", "Git", "GitOps/ArgoCD"]
    },
    {
        "category": "AWS Services",
        "skills": ["Bedrock", "Personalize", "Rekognition", "Neptune", "EC2", "Lambda", "Kinesis"]
    },
    {
        "category": "GenAI / RAG / MLOps",
        "skills": ["AWS Bedrock", "PydanticAI", "LlamaIndex", "MLflow", "Databricks"]
    },
    {
        "category": "Machine Learning",
        "skills": ["GNN", "Recommendation system", "Financial Time-Series", "PyTorch", "Keras", "AWS SageMaker"]
    },
    {
        "category": "Package dependencies management",
        "skills": ["uv", "poetry"]
    }
  ],
  "projects": [
    {
        "id": 1,
        "title": "AI-labeling system for social media platform",
        "description": "Independently designed and developed an end-to-end system for a social media platform, integrating LLM (AWS Bedrock, PydanticAI), Flask, Kafka, and Dagster to create a complete data processing pipeline.",
        "tech": ["Python", "MongoDB", "AWS S3", "LLM", "AWS Bedrock", "PydanticAI", "Flask", "Apache Kafka", "Dagster"],
        "link": ""
    },
    {
        "id": 2,
        "title": "End-to-end data pipeline for Avatar parts",
        "description": "Engineered an end-to-end Avatar parts data pipeline supporting the critical Japan app launch in 2 sprints. This involved coordinating 7 cross-functional teams and tackling the complex challenge of parsing XRS binary files from S3/MongoDB into structured data. All BigQuery transformation logic was governed by dbt for testing and reliability, culminating in a Looker Studio dashboard for PM and Marketing teams.",
        "tech": ["Python", "SQL", "Dagster", "MongoDB", "AWS S3", "BigQuery", "DBT", "Looker Studio"]  
    },
    {
        "id": 3,
        "title": "Scalable Structured Data RAG Workflow for User-Uploaded Assets",
        "description": "Engineered a scalable data ingestion workflow (embedding, chunking, and indexing) using dramatiq for asynchronous processing of user-uploaded CSV files. This process leverages AWS Bedrock Knowledge Bases to enable complex, high-accuracy Q&A capabilities over structured data, enhancing product functionality.",
        "tech": ["Python", "AWS Bedrock Knowledge Bases", "AWS S3", "LLM", "PostgreSQL", "Dramatiq"],
        "link": ""
    },
    {
        "id": 4,
        "title": "Multi-modal RAG system PoC",
        "description": "Researched and applied the latest multi-modal RAG papers, using Llamaindex with Huggingface or large-scale API implementation systems, and developed a baseline multi-modal RAG system based on the Gemini 2.5 series of large models.",
        "tech": ["Python", "LlamaIndex", "MLflow", "Huggingface", "LLM"],
        "link": ""
    },
    {
        "id": 5,
        "title": "Adavanced Structured data RAG system PoC",
        "description": "Using DuckDB with Gemini-2.5-flash-lite, designed a system architecture with highly competitive retrieval time and cost, which can be used immediately after users upload files.",
        "tech": ["Python", "SQL", "PydanticAI", "DuckDB", "LLM", "MLflow"],
        "link": ""
    },
    {
        "id": 6,
        "title": "Text mining for customer feedback data",
        "description": "A NLP model (LDA) was built to model topics, trying to find potential topics or keywords from the original user feedback data, and the prediction results were presented on a dashboard in Tableau.",
        "tech": ["Python", "Airflow", "NLP", "LDA", "Tableau"],
        "link": ""
    },
    {
        "id": 7,
        "title": "Personalized recommendation system",
        "description": "Developed the 'For You' feature of the company's product 'Promeo' from scratch, training a deep learning-based recommendation system through product interaction data to provide personalized recommendations for subscribers.",
        "tech": ["Python", "AWS Personalize", "AWS S3", "Airflow", "AWS DynamoDB", "MLflow", "Superset"],
        "link": ""
    }
  ]
}